# SparkPi.conf (Spark-Bench workload configuration)
spark-bench = {
    spark-submit-config = [
        {
            spark-bench-jar = "/opt/bitnami/spark/benchmark/lib/spark-bench-2.3.0_0.4.0-RELEASE.jar"
            spark-args = {
                deploy-mode = "cluster"
                master = "k8s://https://5b1110ad-1c19-4a94-b088-282ed61e97b1.k8s.ondigitalocean.com"
            }
            # Updated Spark home directory
            spark-home = "/opt/spark"
            conf = {
                # Existing configurations from your command (updated image)
                "spark.kubernetes.executor.request.cores" = "250m"
                "spark.kubernetes.executor.limit.cores" = "500m"
                "spark.executor.memory" = "512m"
                "spark.kubernetes.authenticate.driver.serviceAccountName" = "spark"
                "spark.kubernetes.authenticate.executor.serviceAccountName" = "spark"
                "spark.kubernetes.allocation.pods.allocator" = "statefulset"
                "spark.kubernetes.container.image.pullPolicy" = "Always"
                "spark.kubernetes.container.image" = "truongdh1903/spark-3.5.1d"
                "spark.executor.instances" = "3"
            }
            workload-suites = [
                {
                    descr = "SparkPi Benchmark with Varying Slices"
                    benchmark-output = "console"   # or "json" for structured output
                    # Repeat the benchmark 10 times
                    repeat = 10

                    workloads = [
                        {
                            # Use the workload name from the original command
                            name = "sparkpi"
                            # Define the slices for your benchmark (you had 1000 in the command)
                            slices = [1000] 
                        }
                    ]
                }
            ]
        }
    ]
} 